#include "../../GlobalDefines.cginc"
#include "../DenoisersCommon.cginc"
#include "UnityCG.cginc"
#pragma warning( disable : 3556)

RWTexture2D<uint> ReflRefracAWrite;
Texture2D<uint> ReflRefracA;
Texture2D<uint> ReflRefracB;

float ResRatio;
RWTexture2D<float4> img_quartiles;
RWTexture2D<float4> img_quartiles2;

#define pcadaptive_alpha_reduction 0.5
#define pcadaptive_alpha_ipr_factor 1.5

#define pcfirefly_bias 0.2
#define pcfirefly_ipr_factor 50
float linearstep(const float edge0, const float edge1, const float x) {
  return clamp((x - edge0) / (edge1 - edge0), 0.0, 1.0);
}

inline float4x4 inverse(float4x4 m) {
    float n11 = m[0][0], n12 = m[1][0], n13 = m[2][0], n14 = m[3][0];
    float n21 = m[0][1], n22 = m[1][1], n23 = m[2][1], n24 = m[3][1];
    float n31 = m[0][2], n32 = m[1][2], n33 = m[2][2], n34 = m[3][2];
    float n41 = m[0][3], n42 = m[1][3], n43 = m[2][3], n44 = m[3][3];

    float t11 = n23 * n34 * n42 - n24 * n33 * n42 + n24 * n32 * n43 - n22 * n34 * n43 - n23 * n32 * n44 + n22 * n33 * n44;
    float t12 = n14 * n33 * n42 - n13 * n34 * n42 - n14 * n32 * n43 + n12 * n34 * n43 + n13 * n32 * n44 - n12 * n33 * n44;
    float t13 = n13 * n24 * n42 - n14 * n23 * n42 + n14 * n22 * n43 - n12 * n24 * n43 - n13 * n22 * n44 + n12 * n23 * n44;
    float t14 = n14 * n23 * n32 - n13 * n24 * n32 - n14 * n22 * n33 + n12 * n24 * n33 + n13 * n22 * n34 - n12 * n23 * n34;

    float det = n11 * t11 + n21 * t12 + n31 * t13 + n41 * t14;
    float idet = 1.0f / det;

    float4x4 ret;

    ret[0][0] = t11 * idet;
    ret[0][1] = (n24 * n33 * n41 - n23 * n34 * n41 - n24 * n31 * n43 + n21 * n34 * n43 + n23 * n31 * n44 - n21 * n33 * n44) * idet;
    ret[0][2] = (n22 * n34 * n41 - n24 * n32 * n41 + n24 * n31 * n42 - n21 * n34 * n42 - n22 * n31 * n44 + n21 * n32 * n44) * idet;
    ret[0][3] = (n23 * n32 * n41 - n22 * n33 * n41 - n23 * n31 * n42 + n21 * n33 * n42 + n22 * n31 * n43 - n21 * n32 * n43) * idet;

    ret[1][0] = t12 * idet;
    ret[1][1] = (n13 * n34 * n41 - n14 * n33 * n41 + n14 * n31 * n43 - n11 * n34 * n43 - n13 * n31 * n44 + n11 * n33 * n44) * idet;
    ret[1][2] = (n14 * n32 * n41 - n12 * n34 * n41 - n14 * n31 * n42 + n11 * n34 * n42 + n12 * n31 * n44 - n11 * n32 * n44) * idet;
    ret[1][3] = (n12 * n33 * n41 - n13 * n32 * n41 + n13 * n31 * n42 - n11 * n33 * n42 - n12 * n31 * n43 + n11 * n32 * n43) * idet;

    ret[2][0] = t13 * idet;
    ret[2][1] = (n14 * n23 * n41 - n13 * n24 * n41 - n14 * n21 * n43 + n11 * n24 * n43 + n13 * n21 * n44 - n11 * n23 * n44) * idet;
    ret[2][2] = (n12 * n24 * n41 - n14 * n22 * n41 + n14 * n21 * n42 - n11 * n24 * n42 - n12 * n21 * n44 + n11 * n22 * n44) * idet;
    ret[2][3] = (n13 * n22 * n41 - n12 * n23 * n41 - n13 * n21 * n42 + n11 * n23 * n42 + n12 * n21 * n43 - n11 * n22 * n43) * idet;

    ret[3][0] = t14 * idet;
    ret[3][1] = (n13 * n24 * n31 - n14 * n23 * n31 + n14 * n21 * n33 - n11 * n24 * n33 - n13 * n21 * n34 + n11 * n23 * n34) * idet;
    ret[3][2] = (n14 * n22 * n31 - n12 * n24 * n31 - n14 * n21 * n32 + n11 * n24 * n32 + n12 * n21 * n34 - n11 * n22 * n34) * idet;
    ret[3][3] = (n12 * n23 * n31 - n13 * n22 * n31 + n13 * n21 * n32 - n11 * n23 * n32 - n12 * n21 * n33 + n11 * n22 * n33) * idet;

    return ret;
}

#ifdef HDRP
    Texture2DArray<float> Depth;
#else
    Texture2D<float> Depth;
#endif

#if defined(HDRP) && !defined(TTCustomMotionVectors)
    Texture2DArray<float2> TEX_PT_MOTION;
#else
    Texture2D<float2> TEX_PT_MOTION;
#endif

RWTexture2D<half4> TEX_PT_COLOR_HFWrite;
Texture2D<half4> TEX_PT_COLOR_HF;
RWTexture2D<uint> TEX_PT_COLOR_SPECWrite;
Texture2D<uint> TEX_PT_COLOR_SPEC;
RWTexture2D<half2> IMG_ASVGF_GRAD_HF_SPEC_PING;
RWTexture2D<half2> IMG_ASVGF_GRAD_HF_SPEC_PONG;
Texture2D<half> TEX_PT_VIEW_DEPTH_A;//current frame depth
Texture2D<half> TEX_PT_VIEW_DEPTH_B;//previous frame depth

Texture2D<half4> TEX_ASVGF_HIST_COLOR_HF;
Texture2D<float2> TEX_ASVGF_FILTERED_SPEC_B;
Texture2D<half4> TEX_ASVGF_HIST_MOMENTS_HF_B;
RWTexture2D<half4> IMG_ASVGF_HIST_MOMENTS_HF_A;
RWTexture2D<half4> IMG_ASVGF_ATROUS_PING_HF;
RWTexture2D<uint> IMG_ASVGF_ATROUS_PING_SPEC;
RWTexture2D<float2> IMG_ASVGF_ATROUS_PING_SPEC2;
RWTexture2D<half2> IMG_ASVGF_ATROUS_PING_MOMENTS;

RWTexture2D<half2> MetallicAWrite;
Texture2D<half2> MetallicA;
Texture2D<half2> MetallicB;

Texture2D<half4> TEX_ASVGF_HIST_MOMENTS_HF_A;


RWTexture2D<uint2> TEX_PT_NORMALS_AWrite;
Texture2D<uint2> TEX_PT_NORMALS_A;
Texture2D<uint2> TEX_PT_NORMALS_B;



Texture2D<half4> TEX_ASVGF_ATROUS_PING_HF;
SamplerState my_point_clamp_sampler;

StructuredBuffer<float> ExposureBuffer;
bool UseExposure;
float IndirectBoost;

static const int GRAD_DWN = 3;
#define STRATUM_OFFSET_SHIFT 3
#define STRATUM_OFFSET_MASK ((1 << STRATUM_OFFSET_SHIFT) - 1)

static const float gaussian_kernel[2][2] = {
    { 1.0 / 4.0, 1.0 / 8.0  },
    { 1.0 / 8.0, 1.0 / 16.0 }
};

static const float wavelet_factor = 0.5;
static const float wavelet_kernel[2][2] = {
    { 1.0, wavelet_factor  },
    { wavelet_factor, wavelet_factor * wavelet_factor }
};


#pragma kernel CopyData
Texture2D<float4> PSRGBuff;



Texture2D<int> Normal;

float FarPlane;

int CurFrame;

SamplerState sampler_trilinear_clamp;


Texture2D<uint4> PrimaryTriData;
Texture2D<uint4> SecondaryTriData;


RWTexture2D<uint2> AlbedoColorA;
Texture2D<uint2> AlbedoColorB;



float3 CalcPos(uint4 TriData) {
    if(TriData.w == 99993) return asfloat(TriData.xyz);
    MyMeshDataCompacted Mesh = _MeshData[TriData.x];
    TriData.y += Mesh.TriOffset;
    float4x4 Inverse = inverse(Mesh.W2L);
    return mul(Inverse, float4(AggTrisA[TriData.y].pos0 + asfloat(TriData.z) * AggTrisA[TriData.y].posedge1 + asfloat(TriData.w) * AggTrisA[TriData.y].posedge2,1)).xyz;
}


float3 CalcPos2(uint4 TriData) {
    if(TriData.w == 99993) return asfloat(TriData.xyz);
    MyMeshDataCompacted Mesh = _MeshData[TriData.x];
    TriData.y += Mesh.TriOffset;
    float4x4 Inverse = inverse(Mesh.W2L);
    return mul(Inverse, float4(AggTrisA[TriData.y].pos0 + asfloat(TriData.z) * AggTrisA[TriData.y].posedge1 + asfloat(TriData.w) * AggTrisA[TriData.y].posedge2,1)).xyz;
}

int CalcMat(uint4 TriData) {
    if(TriData.w == 99993) return -1;
    MyMeshDataCompacted Mesh = _MeshData[TriData.x];
    TriData.y += Mesh.TriOffset;
    return AggTrisA[TriData.y].MatDat + Mesh.MaterialOffset;
}


float4x4 viewprojection;
float4x4 prevviewprojection;
int PartialRenderingFactor;
[numthreads(16, 16, 1)]
void CopyData(int3 id : SV_DispatchThreadID)
{
    // if(id.x > screen_width || id.y > screen_height) return;
    // const int pixel_index = id.y * screen_width + id.x;
    // ColData Pixel = GlobalColorsRead[pixel_index];
    // Pixel.Flags = packRGBE(max(unpackRGBE(Pixel.Flags), 0.01f));
    // float3 TexInverted = max(Pixel.Data, 0.01f);
    // float3 TexBaseColor = clamp(TexInverted.xyz,0,12.0f);
    // TexBaseColor = (TexBaseColor > 0.005f ? rcp(TexBaseColor) : 0);
    // float2 MetRough = FromColorSpecPacked(Pixel.MetRoughIsSpec);
    // MetallicAWrite[id.xy] = MetRough;
    // #if defined(HDRP) && !defined(TTCustomMotionVectors)
    //     float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, int3(id.xy,0) / float3(screen_width, screen_height, 1), 0).xy;
    // #else
    //     float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, id.xy / float2(screen_width, screen_height), 0).xy;
    // #endif
    
    // int2 pos_prev = floor((((float2(id.xy)+0.5f) * float2(rcp(screen_width), rcp(screen_height)) + motion) * float2(screen_width, screen_height)));
    // if(any(pos_prev > float2(screen_width, screen_height) || pos_prev < 0)) pos_prev.x = -1000;
    // if(((asuint(ScreenSpaceInfo[id.xy].w) << 4) >> 4) != ((asuint(ScreenSpaceInfoPrev[pos_prev].w) << 4) >> 4)) {
    //     pos_prev.x = -1000;
    // }
    // if(GetBounceData(Pixel.MetRoughIsSpec) == 1) {
    //     Pixel.Direct = float3(0,0,0);
    //     Pixel.Indirect = float3(0,0,0);
    //     Pixel.PrimaryNEERay = 0;
    // }

    // bool ReflectedRefracted = max(FromColorSpecPacked(Pixel.MetRoughIsSpec).z - 2,0);
    // float3 WorldAlbedo = clamp(unpackRGBE(Pixel.Flags),0,12.0f);
    // TexInverted.xyz *= ((WorldAlbedo > 0.001f) ? rcp(WorldAlbedo) : 1.0f);
    // float Exposure = 1;
    // if(UseExposure) Exposure = ExposureBuffer[0];


    // uint2 AlbedoData = asuint(PSRGBuff[id.xy].y);// Pixel.Flags;
    // float4 HFCol = 0;
    // uint SpecCol = 0;
    // float3 Direct = Pixel.Direct + pow(unpackRGBE(Pixel.PrimaryNEERay),2.2f) * TexBaseColor;

    // const float dielectric_specular = 0.04;
    // float3 o_albedo = lerp(unpackRGBE(AlbedoData.x) * (1.0 - dielectric_specular), 0, FromColorSpecPacked(Pixel.MetRoughIsSpec).x * (MetRough.y < 0.6f));
    // float3 o_base_reflectivity = lerp(dielectric_specular, unpackRGBE(AlbedoData.x), FromColorSpecPacked(Pixel.MetRoughIsSpec).x * (MetRough.y < 0.6f));
    //     if(GetBounceData(Pixel.MetRoughIsSpec) != 1) {
    //         AlbedoData.x = packRGBE(o_albedo);
    //         AlbedoData.y = packRGBE(o_base_reflectivity);
    //     }
    // [branch]if (!((asuint(ScreenSpaceInfo[id.xy].w) >> 31) & 0x1) && (FromColorSpecPacked(Pixel.MetRoughIsSpec).z == 2 || MetRough.y >= 0.6f)) {
    //     // if(pos_prev.x != -1000) AlbedoData.y = AlbedoColorB[pos_prev].y;
    //     HFCol = clamp(float4(PartialRenderingFactor * Exposure * (Direct * Pixel.Data.xyz / max(0.01f, o_albedo)),1),0,120000.0f);
    // } else {
    //     // if(pos_prev.x != -1000) AlbedoData.x = AlbedoColorB[pos_prev].x;
    //     SpecCol = packRGBE(PartialRenderingFactor * max(Exposure * (Direct + IndirectBoost * Pixel.Indirect) * Pixel.Data.xyz / max(0.01f, o_base_reflectivity),0));
    // }
    // // ReflectedRefractedTexWrite[id.xy] = (ReflectedRefracted << 31) | ((asuint(ScreenSpaceInfo[id.xy].w) << 4) >> 4);


    // AlbedoColorA[id.xy] = AlbedoData;
    // TEX_PT_COLOR_HFWrite[id.xy] = HFCol;
    // TEX_PT_COLOR_SPECWrite[id.xy] = SpecCol;
    // if(!ReflectedRefracted) {
    //     // if((asuint(ScreenSpaceInfo[id.xy].w) >> 31) & 0x1) TEX_PT_NORMALS_AWrite[id.xy] = uint2(octahedral_32(-i_octahedral_32(asuint(Pixel.throughput.x))), octahedral_32(-i_octahedral_32(asuint(Pixel.throughput.y))));
    //     TEX_PT_NORMALS_AWrite[id.xy] = uint2(asuint(ScreenSpaceInfo[id.xy].xy));//GeoNorm, Norm
    // } else {
    //     TEX_PT_NORMALS_AWrite[id.xy] = TEX_PT_NORMALS_B[pos_prev];
    // }






    if(any(id.xy >= uint2(screen_width, screen_height))) return;
    int pixel_index = id.y * screen_width + id.x;
    ColData Pixel = GlobalColorsRead[pixel_index];
    const uint4 ScreenInfo = asuint(ScreenSpaceInfo[id.xy]);
    float3 TexBaseColor = Pixel.Data.xyz;
    uint2 AlbCol = asuint(PSRGBuff[id.xy].y);
    if(GetBounceData(asuint(PSRGBuff[id.xy].w)) != 1) AlbCol = packRGBE(max(unpackRGBE(AlbCol), 0.01f));
    Pixel.Flags = packRGBE(max(unpackRGBE(asuint(PSRGBuff[id.xy].y)), 0.01f));
    float2 MetRough = FromColorSpecPacked(asuint(PSRGBuff[id.xy].w));
    if(GetBounceData(asuint(PSRGBuff[id.xy].w)) == 1) {
        Pixel.Direct = float3(0,0,0);
        Pixel.Indirect = float3(0,0,0);
        Pixel.PrimaryNEERay = 0;
    }

    #if defined(HDRP) && !defined(TTCustomMotionVectors)
        float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, int3(id.xy,0) / float3(screen_width, screen_height, 1), 0).xy;
    #else
        float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, id.xy / float2(screen_width, screen_height), 0).xy;
    #endif
    
    float2 pos_prev = ((((float2(id.xy)+0.5f) * float2(rcp(screen_width), rcp(screen_height)) + motion) * float2(screen_width, screen_height)));
    bool InvalidReprojection = (any(pos_prev < 0 || pos_prev >= float2(screen_width, screen_height))) || !(((ReflRefracB[pos_prev] << 4) >> 4) == (((ScreenInfo.w << 4) >> 4)));
    bool ReflectedRefracted = max(FromColorSpecPacked(asuint(PSRGBuff[id.xy].w)).z - 2,0);
    Pixel.Data.xyz = max(Pixel.Data.xyz, 0.01f) * rcp(unpackRGBE(Pixel.Flags));
    float3 NEEValue = pow(unpackRGBE(Pixel.PrimaryNEERay),2.2f) * (TexBaseColor > 0.005f ? rcp(TexBaseColor) : 1);

    float Exposure = 1;
    if(UseExposure) Exposure = ExposureBuffer[0];
    float4 HF = 0;
    uint Spec = 0;
    bool IsDiffuse = !((asuint(PSRGBuff[id.xy].w) >> 22) & 0x1) && (FromColorSpecPacked(asuint(PSRGBuff[id.xy].w)).z == 2 || MetRough.y >= 0.6f);


    if (IsDiffuse) {
        if(GetBounceData(asuint(PSRGBuff[id.xy].w)) != 1 && !InvalidReprojection) AlbCol.y = AlbedoColorB[pos_prev].y;
        HF = float4((PartialRenderingFactor) * clamp(Exposure * (Pixel.Direct + Pixel.Indirect + NEEValue) * Pixel.Data.xyz,0,1200000.0f),1);
    } else {
        if(GetBounceData(asuint(PSRGBuff[id.xy].w)) != 1 && !InvalidReprojection) AlbCol.x = AlbedoColorB[pos_prev].x;
        // if((min(GetBounceData(Pixel.MetRoughIsSpec), 3) > 2)) {
        //  Spec = packRGBE((PartialRenderingFactor) * clamp(Exposure * (Pixel.Direct + NEEValue) * Pixel.Data.xyz,0,1200000.0f));
        //  TempSH = irradiance_to_SH(Exposure * (PartialRenderingFactor) * IndirectBoost * clamp(Pixel.Indirect * Pixel.Data.xyz / 1024.0f,0,1200000.0f), asfloat(WorldPosData[id.xy].xyz));
        // } else {
            Spec = packRGBE((PartialRenderingFactor) * clamp(Exposure * (Pixel.Direct + IndirectBoost * Pixel.Indirect + NEEValue) * Pixel.Data.xyz,0,1200000.0f));
        // }
    }
    TEX_PT_COLOR_HFWrite[id.xy] = HF;
    TEX_PT_COLOR_SPECWrite[id.xy] = Spec;
    MetallicAWrite[id.xy] = MetRough;//, ReflectedRefracted, asfloat((asuint(ScreenSpaceInfo[id.xy].w) << 3) >> 3));
    ReflRefracAWrite[id.xy] = (ReflectedRefracted << 31) | ((ScreenInfo.w << 4) >> 4) | ((min(GetBounceData(asuint(PSRGBuff[id.xy].w)), 3) << 29));
    AlbedoColorA[id.xy] = AlbCol;
    TEX_PT_NORMALS_AWrite[id.xy] = ScreenInfo.xy;
    if(!(!ReflectedRefracted || InvalidReprojection)) {
        if(!IsDiffuse) TEX_PT_NORMALS_AWrite[id.xy] = TEX_PT_NORMALS_B[pos_prev].xy;//GeoNorm, Norm
    }




}


uint hash_with(uint seed, uint hash) {
    // Wang hash
    seed = (seed ^ 61) ^ hash;
    seed += seed << 3;
    seed ^= seed >> 4;
    seed *= 0x27d4eb2d;
    return seed;
}
uint pcg_hash(uint seed) {
    uint state = seed * 747796405u + 2891336453u;
    uint word = ((state >> ((state >> 28u) + 4u)) ^ state) * 277803737u;
    return (word >> 22u) ^ word;
}

float2 random(uint samdim, int2 id) {
    uint hash = pcg_hash(((id.x + id.y * screen_width) * (uint)112 + samdim));

    const static float one_over_max_unsigned = asfloat(0x2f7fffff);


    float x = hash_with(CurFrame, hash) * one_over_max_unsigned;
    float y = hash_with(CurFrame + 0xdeadbeef, hash) * one_over_max_unsigned;

    return float2(x, y);

}

int iter;
float CameraDist;
float3 Forward;


#pragma kernel Gradient_Img


inline float get_gradient(float l_curr, float l_prev)
{
    float l_max = max(l_curr, l_prev);

    if (l_max == 0)
        return 0;

    float ret = abs(l_curr - l_prev) / l_max;
    // ret = min(ret, 0.4f);
    ret *= ret; // make small changes less significant

    return ret;
}

inline float2 get_lf_gradient(int2 ipos)
{
    // Find the same surface on the pvreious frame
    #if defined(HDRP) && !defined(TTCustomMotionVectors)
        float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, int3(ipos,0) / float3(screen_width, screen_height, 1), 0).xy;
    #else
        float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, ipos / float2(screen_width, screen_height), 0).xy;
    #endif

    int2 pos_prev = int2(floor(((float2(ipos)+0.5) * float2((rcp(screen_width)), rcp(screen_height)) + motion.xy) * float2(screen_width, screen_height)));

    // Ignore if the surface was outside of the screen
    if (pos_prev.x < 0 || pos_prev.x >= screen_width || pos_prev.y < 0 || pos_prev.y >= screen_height)
        return 0;

    // Get the current path tracer output and the temporally accumulated history.
    // Ignore disocclusion, doesn't seem to be necessary here as there is a huge blur pass
    // done on the LF gradient samples after.
    return 0;
    // float lum_curr = TEX_PT_COLOR_LF_SH[ipos].w;
    // float lum_prev = TEX_ASVGF_HIST_COLOR_LF_SH_B[pos_prev].w;

    // // Return raw colors, do not divide until after the blur pass. We want to detect 
    // // brightness changes over large parts of the screen to avoid noise.
    // return float2(lum_curr, lum_prev);
}
[numthreads(16, 16, 1)]
void Gradient_Img(uint3 gl_GlobalInvocationID : SV_DispatchThreadID)
{
    int2 ipos = gl_GlobalInvocationID.xy;
    if (any((ipos >= int2(screen_width, screen_height) / GRAD_DWN)))
        return;

    float2 grad_lf = 0;

    // Process all LF samples in the 3x3 square, accumulate the luminances

    for (int yy = 0; yy < GRAD_DWN; yy++)
    {
        for (int xx = 0; xx < GRAD_DWN; xx++)
        {
            grad_lf += get_lf_gradient(ipos * GRAD_DWN + int2(xx, yy));
        }
    }

    // IMG_ASVGF_GRAD_LF_PING[ipos] = grad_lf;
}




Texture2D<half2> TEX_ASVGF_GRAD_HF_SPEC_PING;
Texture2D<half2> TEX_ASVGF_GRAD_HF_SPEC_PONG;


#pragma kernel Gradient_Atrous

int iteration;

inline float2 filter_image(Texture2D<half2> img, int2 ipos)
{
    int2 grad_size = int2(screen_width, screen_height) / GRAD_DWN;

    float2 color_center = img[ipos].xy;

    float sum_w = 1;

    const int step_size = int(1u << iteration);

    float2 sum_color = 0;
    sum_w = 0;
    ipos += (random(64, ipos) - 0.5f);

    const int r = 1;
    for (int yy = -r; yy <= r; yy++) {
        for (int xx = -r; xx <= r; xx++) {
            int2 p = ipos + int2(xx, yy) * step_size;

            float2  c = img[p].xy;

            if (any((p >= grad_size) || p < 0))
                c = 0;

            float w = wavelet_kernel[abs(xx)][abs(yy)];// / (float)step_size;

            sum_color += c * w;
            sum_w += w;
        }
    }
    sum_color /= sum_w;


    return sum_color;
}

[numthreads(16, 16, 1)]
void Gradient_Atrous(uint3 id : SV_DispatchThreadID)
{

    int2 ipos = id.xy;
    int2 grad_size = int2(screen_width, screen_height) / GRAD_DWN;
    if (any((ipos >= grad_size) || ipos < 0))
        return;

    float2 filtered_lf = 0;
    float2 filtered_hf_spec = 0;
    [branch] switch (iteration) {
    case 0: filtered_hf_spec = filter_image(TEX_ASVGF_GRAD_HF_SPEC_PING, id.xy); break;
    case 1: filtered_hf_spec = filter_image(TEX_ASVGF_GRAD_HF_SPEC_PONG, id.xy); break;
    case 2: filtered_hf_spec = filter_image(TEX_ASVGF_GRAD_HF_SPEC_PING, id.xy); break;
    case 6:
        break;
    }

    [branch] switch (iteration) {
    case 0: IMG_ASVGF_GRAD_HF_SPEC_PONG[ipos] = filtered_hf_spec; break;
    case 1: IMG_ASVGF_GRAD_HF_SPEC_PING[ipos] = filtered_hf_spec; break;
    case 2: IMG_ASVGF_GRAD_HF_SPEC_PONG[ipos] = filtered_hf_spec; break;
    }

}


#define GROUP_SIZE 15
// spatially compute variance in a 3x3 (radius = 1) or a 5x5 (radius = 2) window 
#define FILTER_RADIUS 1 
// size of the shared memory copies of color, depth, and normals
#define SHARED_SIZE (GROUP_SIZE + FILTER_RADIUS * 2)



#pragma kernel Temporal

groupshared float4 s_normal_lum[SHARED_SIZE][SHARED_SIZE];
groupshared float s_depth[SHARED_SIZE][SHARED_SIZE];





inline void preload(int2 gl_WorkGroupID, int gl_LocalInvocationIndex)
{
    int2 groupBase = gl_WorkGroupID * GROUP_SIZE - FILTER_RADIUS;

    // The size of these shared memory buffers is larger than the group size because we 
    // use them for some spatial filtering. So a single load per thread is not enough.
    // Rename the threads so that some of them will load 2 pixels, and most will load 1 pixel, 
    // in the most dense way possible.
    for (uint linear_idx = gl_LocalInvocationIndex; linear_idx < SHARED_SIZE * SHARED_SIZE; linear_idx += GROUP_SIZE * GROUP_SIZE)
    {
        // Convert the linear index to 2D index in a SHARED_SIZE x SHARED_SIZE virtual group
        float t = (float(linear_idx) + 0.5) / float(SHARED_SIZE);
        int xx = int(floor(frac(t) * float(SHARED_SIZE)));
        int yy = int(floor(t));

        // Load
        int2 ipos = groupBase + int2(xx, yy);
        float depth = TEX_PT_VIEW_DEPTH_A[ipos].x;
        float3 normal = i_octahedral_32(TEX_PT_NORMALS_A[ipos].y);
        float3 color_hf = IMG_ASVGF_ATROUS_PING_HF[ipos];

        // Store
        s_normal_lum[yy][xx] = float4(normal.xyz, luminance(color_hf.rgb));//packHalf4x16(float4(normal.xyz, luminance(color_hf.rgb)));
        s_depth[yy][xx] = depth;
    }
}


inline void get_shared_data(int2 offset, out float depth, out float3 normal, out float lum_hf, int2 gl_LocalInvocationID)
{
    int2 addr = gl_LocalInvocationID + int2(FILTER_RADIUS, FILTER_RADIUS) + offset;

    float4 normal_lum = s_normal_lum[addr.y][addr.x];
    depth = s_depth[addr.y][addr.x];

    normal = normal_lum.xyz;
    lum_hf = normal_lum.w;
}

float3 CamDelta;

Texture2D<float4> quart_read;
Texture2D<float4> quart_read2;


bool reprojection_intersect_border(inout float2 prev_pos, const float2 mv, const float2 image_size_minus_one) {
    if (any(round(prev_pos) > image_size_minus_one) || any(round(prev_pos) < 0)) {
        // Intersect motion vector with image:
        const float tmin = max(min(prev_pos.x / mv.x, (prev_pos.x - image_size_minus_one.x) / mv.x),
                               min(prev_pos.y / mv.y, (prev_pos.y - image_size_minus_one.y) / mv.y));
        prev_pos = prev_pos - mv * tmin;
        return true;
    }
    return false;
}


[numthreads(GROUP_SIZE, GROUP_SIZE, 1)]
void Temporal(uint3 gl_GlobalInvocationID : SV_DispatchThreadID, uint3 gl_WorkGroupID : SV_GroupID, uint gl_LocalInvocationIndex : SV_GroupIndex, uint3 gl_LocalInvocationID : SV_GroupThreadID)
{
    preload(gl_WorkGroupID, gl_LocalInvocationIndex);
    GroupMemoryBarrierWithGroupSync();

    int2 ipos = gl_GlobalInvocationID.xy;
    #if defined(HDRP) && !defined(TTCustomMotionVectors)
        float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, int3(ipos,0) / float3(screen_width, screen_height, 1), 0).xy;
    #else
        float2 motion = -TEX_PT_MOTION.SampleLevel(my_linear_clamp_sampler, ipos / float2(screen_width, screen_height), 0).xy;
    #endif


    // float2 pos_prev = ((((float2(ipos)+0.5f) * float2(rcp(screen_width), rcp(screen_height)) + motion.xy) * float2(screen_width, screen_height)));// + (random(54, gl_GlobalInvocationID) - 0.5f) * 0.2f;

    bool Lowered = false;
    float2 pos_prev = ipos.xy + 0.5f + motion * float2(screen_width, screen_height);
    Lowered = reprojection_intersect_border(pos_prev, motion, int2(screen_width, screen_height) - 1);


    // Load the parameters of the target pixel
    float depth_curr;
    float3 normal_curr;
    float lum_curr_hf;
    get_shared_data(0, depth_curr, normal_curr, lum_curr_hf, gl_LocalInvocationID);
    Ray ray = CreateCameraRay(ipos / float2(screen_width, screen_height) * 2.0f - 1.0f);
    float4 curprojectedrefl = mul(viewprojection, float4(ray.origin + ray.direction * depth_curr, 1));
    float4 prevprojectedrefl = mul(prevviewprojection, float4(ray.origin + ray.direction * depth_curr, 1));
    float2 reflprojection = ((curprojectedrefl.xy / curprojectedrefl.w) - (prevprojectedrefl.xy / prevprojectedrefl.w)) * 0.5f;

    float motion_length = length((motion.xy + reflprojection) * float2(screen_width, screen_height));
    // float motion_length = length((motion.xy) * float2(screen_width, screen_height));
    
    float4 percentiles;

    const uint2 rounded_size = uint2(screen_width, screen_height);
    percentiles = quart_read.SampleLevel(my_linear_clamp_sampler, ((float2)(ipos.xy) + 0.5) / (float2)rounded_size,0);
    float4 percentiles2 = quart_read2.SampleLevel(my_linear_clamp_sampler, ((float2)(ipos.xy) + 0.5) / (float2)rounded_size,0);

    // float CenterFDepth = TEX_PT_VIEW_DEPTH_A[ipos].y;

    float shininess = clamp(2.0 / max(pow(MetallicA[ipos].y, 4), 0.001f) - 2.0, 0.0, 32.0);

    float3 geo_normal_curr = i_octahedral_32(TEX_PT_NORMALS_A[ipos].x);

    bool ReflectedRefracted = ReflRefracA[gl_GlobalInvocationID.xy] >> 31;

    uint MatIndex = (ReflRefracA[gl_GlobalInvocationID.xy] << 4) >> 4;

    // Try to get the history sample for all channels, including HF moments
    bool temporal_sample_valid_diff = false;
    bool temporal_sample_valid_spec = false;
    float3 temporal_color_hf = 0;
    float4 temporal_moments_histlen_hf = 0;
    float4 temporal_color_histlen_spec = 0;
        float temporal_sum_w_diff = 0.0;
        float temporal_sum_w_spec = 0.0;

        float2 pos_ld = floor(pos_prev - 0.5);
        float2 subpix = frac(pos_prev - 0.5 - pos_ld);
    {

        // Bilinear/bilateral filter
        static const int2 off[4] = { { 0, 0 }, { 1, 0 }, { 0, 1 }, { 1, 1 } };
        const float w[4] = {
            (1.0 - subpix.x) * (1.0 - subpix.y),
            (subpix.x) * (1.0 - subpix.y),
            (1.0 - subpix.x) * (subpix.y),
            (subpix.x) * (subpix.y)
        };
        [unroll]for (int i = 0; i < 4; i++) {
            int2 p = int2(pos_ld)+off[i];

            if (p.x < 0 || p.x >= screen_width || p.y >= screen_height)
                continue;

            float depth_prev = TEX_PT_VIEW_DEPTH_B[p].x;
            uint2 norms = TEX_PT_NORMALS_B[p];
            float3  normal_prev = i_octahedral_32(norms.y);
            float3  geo_normal_prev = i_octahedral_32(norms.x);//needs to be TEX_PT_GEO_NORMAL_B, but since for now I am not worrying about normal maps yet, it can use the same texture
            bool ReflectedRefractedPrev = ReflRefracB[p] >> 31;
            

            float dist_depth = (abs(depth_curr - depth_prev) - CameraDist) / depth_curr;// * max(CenterFDepth,0.25f);
            float dot_normals = dot(normal_curr, normal_prev);
            float dot_geo_normals = dot(geo_normal_curr, geo_normal_prev);
            if(MatIndex == ((ReflRefracB[p] << 4) >> 4))
            [branch]if(!ReflectedRefracted && !ReflectedRefractedPrev) {
                if (dist_depth < 0.1)
                {
                    float w_diff = w[i] * max(dot_normals, 0);
                    float w_spec = w[i] * pow(max(dot_normals, 0), shininess);

    

                    temporal_color_hf += TEX_ASVGF_HIST_COLOR_HF[p] * w_diff;
                    temporal_color_histlen_spec += float4(unpackRGBE(asuint(TEX_ASVGF_FILTERED_SPEC_B[p].x)), TEX_ASVGF_FILTERED_SPEC_B[p].y) * w_spec;
                    float4 A = TEX_ASVGF_HIST_MOMENTS_HF_B[p].rgba;
                    temporal_moments_histlen_hf += A * w_diff;
                    temporal_sum_w_diff += w_diff;
                    temporal_sum_w_spec += w_spec;
                }
            } else {
                if (dist_depth < 0.1)
                {
                    float w_diff = pow(w[i],clamp(6.0f / depth_curr,1.0f, 3.0f));
                    float w_spec = w[i];


                    temporal_color_hf += TEX_ASVGF_HIST_COLOR_HF[p] * w_diff;
                    temporal_color_histlen_spec += float4(unpackRGBE(asuint(TEX_ASVGF_FILTERED_SPEC_B[p].x)), TEX_ASVGF_FILTERED_SPEC_B[p].y) * w_spec;
                    temporal_moments_histlen_hf += TEX_ASVGF_HIST_MOMENTS_HF_B[p].rgba * w_diff;
                    temporal_sum_w_diff += w_diff;
                    temporal_sum_w_spec += w_spec;
                }   
            }
        }

        // We found some relevant surfaces - good
        if (temporal_sum_w_diff > 1e-6)
        {
            float inv_w_diff = 1.0 / temporal_sum_w_diff;
            temporal_color_hf *= inv_w_diff;
            temporal_moments_histlen_hf *= inv_w_diff;
            temporal_sample_valid_diff = true;
        }

        if (temporal_sum_w_spec > 1e-6)
        {
            float inv_w_spec = 1.0 / temporal_sum_w_spec;
            temporal_color_histlen_spec *= inv_w_spec;
            temporal_sample_valid_spec = true;
        }

    }

    if(Lowered) {
        temporal_color_histlen_spec.w = 1;
        temporal_moments_histlen_hf.ab = 1;
    }

    // Compute spatial moments of the HF channel in a 3x3 window
    float2 spatial_moments_hf = float2(lum_curr_hf, lum_curr_hf * lum_curr_hf);

    {
        float spatial_sum_w_hf = 1.0;
        for (int yy = -1; yy <= 1; yy++) {
            for (int xx = -1; xx <= 1; xx++) {
                if (xx == 0 && yy == 0)
                    continue;

                int2 p = ipos + int2(xx, yy);

                float depth;
                float3 normal;
                float lum_p_hf;
                get_shared_data(int2(xx, yy), depth, normal, lum_p_hf, gl_LocalInvocationID);
                // lum_p_hf += luminance(unpackRGBE(TEX_PT_COLOR_SPEC[p].x));
                float dist_z = abs(depth_curr - depth) / abs(depth_curr);// * CenterFDepth;// * FDepth[ipos] * 120.0f;
                if (dist_z < 2.0) {
                    float w_hf = clamp(pow(max(0.0, dot(normal, normal_curr)), 128.0),0,1);

                    spatial_moments_hf += float2(lum_p_hf * w_hf, lum_p_hf * lum_p_hf * w_hf);
                    spatial_sum_w_hf += w_hf;
                }
            }
        }

        float inv_w2_hf = 1.0f / spatial_sum_w_hf;
        spatial_moments_hf *= inv_w2_hf;
    }


    float3 color_curr_spec = unpackRGBE(TEX_PT_COLOR_SPEC[ipos].x);
    float2 spatial_moments_spec = float2(luminance(color_curr_spec), luminance(color_curr_spec) * luminance(color_curr_spec));

    {
        float spatial_sum_w_spec = 1.0;
        for (int yy = -4; yy <= 4; yy++) {
            for (int xx = -4; xx <= 4; xx++) {
                if (xx == 0 && yy == 0)
                    continue;

                int2 p = ipos + int2(xx, yy);

                float depth;
                float3 normal;
                float lum_p_hf;
                // get_shared_data(int2(xx, yy), depth, normal, lum_p_hf, gl_LocalInvocationID);
                // lum_p_hf += luminance(unpackRGBE(TEX_PT_COLOR_SPEC[p].x));
                float dist_z = abs(depth_curr - TEX_PT_VIEW_DEPTH_A[p].x) / abs(depth_curr);// * CenterFDepth;// * FDepth[ipos] * 120.0f;
                if (dist_z < 2.0) {
                    float w_hf = clamp(pow(max(0.0, dot( i_octahedral_32(TEX_PT_NORMALS_A[p].y), normal_curr)), 128.0),0,1);

                    float lum_p_spec = luminance(unpackRGBE(TEX_PT_COLOR_SPEC[p].x));
                    spatial_moments_spec += float2(lum_p_spec * w_hf, lum_p_spec * lum_p_spec * w_hf);
                    spatial_sum_w_spec += w_hf;
                }
            }
        }

        float inv_w2_hf = 1.0f / spatial_sum_w_spec;
        spatial_moments_spec *= inv_w2_hf;
    }

    // Load the target pixel colors for all channels
    float3 color_curr_hf = IMG_ASVGF_ATROUS_PING_HF[ipos];

 {
    float max_l = 99999999.0f;
    // if (pc.firefly_filter_enable != 0) {
      max_l = min(max_l, pcfirefly_bias + percentiles.y + pcfirefly_ipr_factor * (percentiles.y - percentiles.x));
    // }

    if (spatial_moments_hf.x /* = yuv_luminance(irr)*/ > max_l) {
      const float factor = max_l / spatial_moments_hf.x;
      color_curr_hf *= factor;
      spatial_moments_hf *= float2(factor, factor * factor);
    }
  }

 {
    float max_l = 99999999.0f;
    // if (pc.firefly_filter_enable != 0) {
      max_l = min(max_l, pcfirefly_bias + percentiles2.y + pcfirefly_ipr_factor * (percentiles2.y - percentiles2.x));
    // }

    if (spatial_moments_spec.x /* = yuv_luminance(irr)*/ > max_l) {
      const float factor = max_l / spatial_moments_spec.x;
      color_curr_spec *= factor;
      spatial_moments_spec *= float2(factor, factor * factor);
    }
  }


    float3 out_color_hf;
    float4 out_color_histlen_spec;
    float4 out_moments_histlen_hf;

    // Load the gradients
    float2 grad_hf_spec = 0;//TEX_ASVGF_GRAD_HF_SPEC_PONG[ipos / GRAD_DWN].rg;

    if (temporal_sample_valid_diff)
    {
        // Compute the antilag factors based on the gradients
        float antilag_alpha_hf = clamp(lerp(1.0, 1.0f * grad_hf_spec.x, 1.0f), 0, 1);//play with the middle 2 values?


        const float ipr = pcadaptive_alpha_ipr_factor * (percentiles.w - percentiles.z);
        float adaptive_alpha = (1.0 - pcadaptive_alpha_reduction * linearstep(percentiles.w, percentiles.w + ipr, temporal_moments_histlen_hf.x));
        adaptive_alpha      *= (1.0 - pcadaptive_alpha_reduction * (1.0 - linearstep(percentiles.z - ipr, percentiles.z, temporal_moments_histlen_hf.x)));

        temporal_moments_histlen_hf.ba = min(1.0 / (1.0 - adaptive_alpha) - 1, temporal_moments_histlen_hf.ba);

        // Adjust the history length, taking the antilag factors into account
        float hist_len_hf = min(temporal_moments_histlen_hf.b * pow(1.0 - antilag_alpha_hf, 10) + 1.0, 256.0);

        // Compute the blending weights based on history length, so that the filter
        // converges faster. I.e. the first frame has weight of 1.0, the second frame 1/2, third 1/3 and so on.
        float alpha_color_hf = max(0.02f, 1.0f / hist_len_hf);
        float alpha_moments_hf = max(0.01f, 1.0f / hist_len_hf);

        // Adjust the blending factors, taking the antilag factors into account again
        alpha_color_hf = lerp(alpha_color_hf, 1.0, antilag_alpha_hf);
        alpha_moments_hf = lerp(alpha_moments_hf, 1.0, antilag_alpha_hf);

        // Blend!
        out_color_hf.rgb = lerp(temporal_color_hf.rgb, color_curr_hf.rgb, alpha_color_hf);

        out_moments_histlen_hf.rg = lerp(temporal_moments_histlen_hf.rg, spatial_moments_hf.rg, alpha_moments_hf);
        out_moments_histlen_hf.b = hist_len_hf;
    }
    else
    {
        // No valid history - just use the current color and spatial moments
        out_color_hf.rgb = color_curr_hf;
        out_moments_histlen_hf = float4(spatial_moments_hf, 1, 1);
    }

    if (temporal_sample_valid_spec)
    {
        // float3 Max = 0;
        // for(int x = -1; x <= 1; x++) {
        //     for(int y = -1; y <= 1; y++) {
        //         if(x == 0 && y == 0) continue;
        //         float3 col = unpackRGBE(TEX_PT_COLOR_SPEC[ipos + int2(x,y)]);
        //         Max = max(Max, col);
        //     }
        // }
        // color_curr_spec.rgb = exp(clamp(log(color_curr_spec.rgb + 1), 0, log(Max + 15))) - 1;

        // Same sequence as above, only for the specular channel
        float antilag = grad_hf_spec.y * clamp(1.0f - motion_length, 0.1f, 1.0f) + motion_length * 0.001f;
        // float antilag = grad_hf_spec.y + motion_length * 0.001f;
        float antilag_alpha_spec = clamp(lerp(0.0, antilag, 1), 0, 1);


        const float ipr = pcadaptive_alpha_ipr_factor * (percentiles2.w - percentiles2.z);
        float adaptive_alpha = (1.0 - pcadaptive_alpha_reduction * linearstep(percentiles2.w, percentiles2.w + ipr, luminance(temporal_color_histlen_spec.rgb)));
        adaptive_alpha      *= (1.0 - pcadaptive_alpha_reduction * (1.0 - linearstep(percentiles2.z - ipr, percentiles2.z, luminance(temporal_color_histlen_spec.rgb))));

        temporal_color_histlen_spec.a = min(1.0 / (1.0 - adaptive_alpha) - 1, temporal_color_histlen_spec.a);


        float hist_len_spec = min(temporal_color_histlen_spec.a + 1.0, 256.0);
        float alpha_color_spec = max(0.1f, 1.0 / hist_len_spec);
        alpha_color_spec = lerp(alpha_color_spec, 1.0, antilag_alpha_spec);
        out_color_histlen_spec.rgb = lerp(temporal_color_histlen_spec.rgb, color_curr_spec.rgb, alpha_color_spec);
        out_color_histlen_spec.a = hist_len_spec;
    }
    else
    {
        out_color_histlen_spec = float4(color_curr_spec, 1);
    }


    // Store the outputs for furhter processing by the a-trous HF filter
    IMG_ASVGF_HIST_MOMENTS_HF_A[ipos] = out_moments_histlen_hf;
    IMG_ASVGF_ATROUS_PING_HF[ipos] = float4(out_color_hf,1);
    IMG_ASVGF_ATROUS_PING_SPEC2[ipos] = float2(asfloat(packRGBE(out_color_histlen_spec)), out_color_histlen_spec.a);
    IMG_ASVGF_ATROUS_PING_MOMENTS[ipos] = out_moments_histlen_hf.xy;

    // GroupMemoryBarrierWithGroupSync();

    // Store the LF channel into shared memory for averaging

}





#pragma kernel Atrous


bool plane_distance_disocclusion_check(float3 current_pos, float3 history_pos, float3 current_normal)
{
    float3  to_current    = current_pos - history_pos;
    float dist_to_plane = abs(dot(to_current, current_normal));

    return dist_to_plane > 0.01f;
}
uniform int spec_iteration;

float square(float x) { return x * x; }

// Converts a square of roughness to a Phong specular power
float RoughnessSquareToSpecPower(in float alpha) {
    return max(0.01, 2.0f / (square(alpha) + 1e-4) - 2.0f);
}
bool plane_distance_disocclusion_check(float3 current_pos, float3 history_pos, float3 current_normal, float dist_z)
{
    float3  to_current    = current_pos - history_pos;
    float dist_to_plane = abs(dot(to_current, current_normal));

    return dist_to_plane > 0.01f * dist_z;
}
inline void filter_image(
    Texture2D<half4> img_hf,
    Texture2D<uint> img_spec,
    Texture2D<half2> img_moments,
    out float3 filtered_hf,
    out float3 filtered_spec,
    out float2 filtered_moments,
    int2 gl_GlobalInvocationID)
{
    int2 ipos = int2(gl_GlobalInvocationID);

    float3 color_center_hf = img_hf[ipos];
    float3 color_center_spec = unpackRGBE(img_spec[ipos]);
    float2 moments_center = img_moments[ipos].xy;

    filtered_hf = color_center_hf;
    filtered_spec = color_center_spec;
    filtered_moments = moments_center;

    float depth_center = PSRGBuff[ipos].z;
    // float fwidth_depth = TEX_PT_VIEW_DEPTH_A[ipos].y;
    Ray CenterRay = CreateCameraRay((float2)ipos / float2(screen_width, screen_height) * 2.0f - 1.0f);
    float3 CenterPos = CenterRay.direction * depth_center + CenterRay.origin;


    float lum_mean_hf = luminance(color_center_hf);
    float sigma_l_hf = 0;

    float hist_len_hf = TEX_ASVGF_HIST_MOMENTS_HF_A[ipos].b;
    float3 normal_center = normalize(i_octahedral_32(asuint(PSRGBuff[ipos].x)));
    float step_size = (1u << (spec_iteration));
    if(spec_iteration > 0) step_size = (1u << (6 - spec_iteration));
    if(spec_iteration != 1) {//should be 4?
        float3 Min = 9999.0f;
        float3 Max = -9999.0f;
        int ValidTaps = 0;
        [unroll]for(int i = -1; i <= 1; i++) {
            [unroll]for(int j = -1; j <= 1; j++) {
                int2 offID = ipos + int2(i, j) * step_size;
                if(i == 0 && j == 0) continue;
                float3 Val = unpackRGBE(img_spec[offID]);
                if(luminance(Val) != 0) {
                    ValidTaps++;
                }
                    Min = min(Min, Val);
                    Max = max(Max, Val);
            }
        }
        filtered_spec = clamp(filtered_spec, Min, Max);
    }
    // [branch]if (hist_len_hf > 2 || spec_iteration == 0) {
        // Compute luminance variance from the statistical moments: Var(X) = E[X^2] - E[X]^2
        // The `asvgf_temporal` shader computes a combination of temporal and spatial (3x3) moments,
        // and stores these into a texture. This shader will combine moments of the surrounding 
        // pixels using the same weights as for colors, and the combined moments are used on the next iteration.
        lum_mean_hf = filtered_moments.x;
        float lum_variance_hf = max(1e-8, filtered_moments.y - filtered_moments.x * filtered_moments.x);
        sigma_l_hf = min(max(hist_len_hf, 1), 16) / (2.0 * lum_variance_hf);
        // - (spec_iteration == 0 ? 0 : 2)
    // } else {
    // //  // If there is no history, treat all moments as invalid, because 3x3 spatial 
    // //  // is just not enough to get reasonable filtering. Ignore luminance in this case,
    // //  // and perform a depth-normal-guided bilateral blur.
    //     lum_mean_hf = filtered_moments.x;
    //     sigma_l_hf = 0.005f;
    // }

    // reduce the HF filter sensitivity to normals when the lighting is invalidated
 float normal_weight_scale = clamp(hist_len_hf / 8.0f, 0, 1);

    float normal_weight_hf = 12;
    const float CenterMetallic = MetallicA[ipos].x;
    const float roughness_center = MetallicA[ipos].y;
    normal_weight_hf *= normal_weight_scale;
    float normal_weight_spec = RoughnessSquareToSpecPower(square(roughness_center));
    normal_weight_spec = clamp(normal_weight_spec, 8, 1024);
    normal_weight_spec *= normal_weight_scale;




    float sum_w_hf = 1.0;
    float sum_w_spec = 1.0;

    // Add some jitter to sample positions to hide the a-trous filter aliasing patterns
    // [branch] if(spec_iteration == 4) ipos += int2((random(spec_iteration, gl_GlobalInvocationID.xy) - 0.5f) * step_size);

    float spec_filter_width_scale = clamp(roughness_center * 30 - spec_iteration, 0.1f, 1);


    // Compute the weighted average of color and moments from a sparse 3x3 pattern around the target pixel
    [loop]for (int yy = -1; yy <= 1; yy++) {
        [unroll]for (int xx = -1; xx <= 1; xx++) {

            if (xx == 0 && yy == 0)
                continue;
            int2 p = ipos + int2(xx, yy) * step_size;// + jitter;
            int actxx = xx;
            int actyy = yy;
            if(p.x <= 0 || p.x > screen_width) {
                actxx *= -1;
            }
            if(p.y <= 0 || p.y > screen_height) {
                actyy *= -1;
            }
            p = ipos + int2(actxx, actyy) * step_size;// + jitter;

            float w = float(all((p >= 0))
                && all((p < int2(screen_width, screen_height))));
            if(w == 0) continue;

            float3 normal = normalize(i_octahedral_32(asuint(PSRGBuff[p].x)));

            float depth = PSRGBuff[p].z;


            Ray ray = CreateCameraRay((float2)p / float2(screen_width, screen_height) * 2.0f - 1.0f);
            float3 pos = ray.direction * depth + ray.origin;

            float dist_z = abs(depth_center - depth);
            // w *= lerp(!plane_distance_disocclusion_check(CenterPos, pos, normal_center), exp(-dist_z), saturate(step_size / 20.0f));
            w *= !plane_distance_disocclusion_check(CenterPos, pos, normal_center, pow((1.0f + dist_z) * sqrt(step_size), 2.0f));// * (exp(-abs(depth - depth_center) / depth_center));
            // w *= !plane_distance_disocclusion_check(CenterPos, pos, normal_center);// * (exp(-abs(depth - depth_center) / depth_center));
            w *= wavelet_kernel[abs(xx)][abs(yy)];

            // float dist_z = abs(depth_center - depth);// * fwidth_depth;
            // w *= exp(-dist_z);
            // w *= wavelet_kernel[abs(xx)][abs(yy)];

            float w_hf = w;

            float3 c_hf = img_hf[p];
            float3 c_spec = unpackRGBE(img_spec[p]);
            float2 c_mom = img_moments[p].xy;
            float l_hf = luminance(c_hf.rgb);
            float dist_l_hf = abs(lum_mean_hf - l_hf);

            w_hf *= exp(-dist_l_hf * dist_l_hf * sigma_l_hf);

            w_hf *= 1.0f - clamp(abs(MetallicA[p].x - CenterMetallic),0,1);
            float w_spec = w_hf;

            w_spec *= spec_filter_width_scale;


            float NdotN = max(0.0, dot(normal_center, normal));

            if (normal_weight_hf > 0)
            {
                w_hf *= clamp(pow(NdotN, normal_weight_hf),0,1);
            }

            if (normal_weight_spec > 0)
            {
                w_spec *= pow(NdotN, normal_weight_spec);
            }
            // w_spec *= pow(clamp(abs(MetallicA[p].x - CenterMetallic), 0, 1), step_size);
            if(spec_iteration != 0)  w_spec *= pow(1.0f - length(ray.direction - CenterRay.direction) / abs(depth_center * depth_center), 32.0f * step_size);

            filtered_hf += c_hf.rgb * w_hf;
            filtered_spec += c_spec.rgb * w_spec;
            filtered_moments += c_mom * w_hf;
            sum_w_hf += w_hf;
            sum_w_spec += w_spec;
        }
    }

    filtered_hf = filtered_hf / sum_w_hf;
    filtered_spec = filtered_spec / sum_w_spec;
    filtered_moments = filtered_moments / sum_w_hf;
}



float3 composite_color(float4 surf_base_color, float3 high_freq, float3 specular, float3 SpecColor, int2 ipos)
{
    float Exposure = 1;
    if(UseExposure) Exposure = ExposureBuffer[0];
    float3 final_color = ((surf_base_color.w == 1) ? pow(surf_base_color, 1.0f) : ((high_freq.rgb) * surf_base_color.xyz / Exposure + specular * SpecColor / Exposure));

    return final_color;
}
int MaxIterations;

Texture2D<half2> TEX_ASVGF_ATROUS_PING_MOMENTS;
Texture2D<uint> TEX_ASVGF_ATROUS_PING_SPEC;
RWTexture2D<float4> IMG_ASVGF_COLOR;

[numthreads(16, 16, 1)]
void Atrous(uint3 gl_GlobalInvocationID : SV_DispatchThreadID, uint3 gl_WorkGroupID : SV_GroupID, uint gl_LocalInvocationIndex : SV_GroupIndex, uint3 gl_LocalInvocationID : SV_GroupThreadID)
{
    int2 ipos = gl_GlobalInvocationID.xy;
    if (any((ipos >= int2(screen_width, screen_height))))
        return;

    float3 filtered_hf;
    float3 filtered_spec;
    float2 filtered_moments;

    filter_image(TEX_ASVGF_ATROUS_PING_HF, TEX_ASVGF_ATROUS_PING_SPEC, TEX_ASVGF_ATROUS_PING_MOMENTS, filtered_hf, filtered_spec, filtered_moments, ipos);
    

    if(spec_iteration < 4) {
        IMG_ASVGF_ATROUS_PING_HF[ipos] = float4(filtered_hf,1);
        IMG_ASVGF_ATROUS_PING_SPEC[ipos] = packRGBE(filtered_spec);
        IMG_ASVGF_ATROUS_PING_MOMENTS[ipos] = filtered_moments;
    }

    // Perform compositing on the last iteration
    if (spec_iteration == MaxIterations)
    {


        float3 normal = i_octahedral_32(TEX_PT_NORMALS_A[ipos].y);
        float4 base_color = float4(unpackRGBE(AlbedoColorB[ipos].x),GetBounceData(GlobalColorsRead[ipos.x + ipos.y * screen_width].MetRoughIsSpec));// TEX_PT_BASE_COLOR_A.SampleLevel(sampler_trilinear_clamp, ipos / float2(screen_width, screen_height), 0);
        if(UpscalerMethod != 0 && DiffRes && base_color.w <= 2) base_color.xyz = 1;
        if(base_color.w == 1) base_color = float4(GlobalColorsRead[ipos.x + ipos.y * screen_width].Data,1);
        // Project the spherical harmonics lighting onto the actual surface normal
        
        float3 final_color = composite_color(base_color, filtered_hf, filtered_spec, (UpscalerMethod != 0 && DiffRes && base_color.w <= 2) ? 1 : unpackRGBE(AlbedoColorB[ipos].y), ipos);

        IMG_ASVGF_COLOR[ipos] = float4(final_color, 0);
    
    }
} 

float NearPlane;
#pragma kernel DistanceCorrectionKernel
float FetchDepth(float2 UV) {
    Ray ray = CreateCameraRay(UV * 2.0f - 1.0f);  
    #ifdef HDRP
        float depth = 1.0f - (Depth[int3(UV * float2(screen_width, screen_height),0)].x);
    #else
        float depth = 1.0f - (Depth.SampleLevel(my_linear_clamp_sampler, UV, 0).x);
    #endif
        depth = NearPlane * FarPlane / (depth * (NearPlane - FarPlane) + FarPlane);
        return length(ray.direction / dot(ray.direction, Forward) * depth);   
}

RWTexture2D<half2> TEX_PT_VIEW_DEPTH_AWRITE;//current frame depth
[numthreads(32, 32, 1)]
void DistanceCorrectionKernel(uint3 id : SV_DispatchThreadID) {
    if(id.x >= (uint)screen_width || id.y >= (uint)screen_height) return;
    float2 Uv = (id.xy + 0.5f) / float2(screen_width, screen_height);

    float CurDepth = FetchDepth(Uv);
    float CurDepthX = FetchDepth(Uv + float2(rcp(screen_width) * 2.0f, 0));
    float CurDepthY = FetchDepth(Uv + float2(0, rcp(screen_height) * 2.0f));
    float CurDepthX2 = FetchDepth(Uv - float2(rcp(screen_width) * 2.0f, 0));
    float CurDepthY2 = FetchDepth(Uv - float2(0, rcp(screen_height) * 2.0f));
    TEX_PT_VIEW_DEPTH_AWRITE[id.xy] = CurDepth;//, abs(CurDepth - CurDepthX) + abs(CurDepth - CurDepthY) + abs(CurDepth - CurDepthY2) + abs(CurDepth - CurDepthX2));
}

#pragma kernel TempCopyKernel

[numthreads(32, 32, 1)]
void TempCopyKernel(uint3 id : SV_DispatchThreadID) {
    IMG_ASVGF_ATROUS_PING_SPEC[id.xy] = asuint(TEX_ASVGF_FILTERED_SPEC_B[id.xy].x);
}


groupshared float sortDiff[64];
groupshared float sortSpec[64];
#define Swap(A, B) {float C = B; B = A; A = C;}

#pragma kernel CalcPerc
[numthreads(8, 8, 1)]
void CalcPerc(int3 id : SV_DispatchThreadID, int GTI : SV_GroupIndex, int3 GroupID : SV_GroupID)
{
  const int2 pixel = min(id.xy, int2(screen_width, screen_height));

  // Sort luminance using odd even sort
  sortDiff[GTI] = luminance(TEX_PT_COLOR_HF[pixel]);
  sortSpec[GTI] = luminance(unpackRGBE(TEX_PT_COLOR_SPEC[pixel]));
  GroupMemoryBarrierWithGroupSync();
    const bool Valid = GTI < 63;
    const bool OddSorter = GTI % 2 == 0;
    const bool EvenSorter = GTI % 2 == 1;

    [unroll]for (int i = 0; i < 32; i++) {
      if(Valid)
      if (OddSorter) {
        if (sortDiff[GTI] > sortDiff[GTI + 1]) Swap(sortDiff[GTI], sortDiff[GTI + 1]);
        if (sortSpec[GTI] > sortSpec[GTI + 1]) Swap(sortSpec[GTI], sortSpec[GTI + 1])
      }
      GroupMemoryBarrierWithGroupSync();
      
      if(Valid)
      if (EvenSorter) {
        if (sortDiff[GTI] > sortDiff[GTI + 1]) Swap(sortDiff[GTI], sortDiff[GTI + 1]);
        if (sortSpec[GTI] > sortSpec[GTI + 1]) Swap(sortSpec[GTI], sortSpec[GTI + 1])
      }
      GroupMemoryBarrierWithGroupSync();
    }


  if (GTI == 0) {
    float firefly_lower = sortDiff[16];
    float firefly_upper = sortDiff[57];
    float adaptive_alpha_lower = sortDiff[1];
    float adaptive_alpha_upper = sortDiff[60];
    img_quartiles[GroupID.xy] = float4(firefly_lower, firefly_upper, adaptive_alpha_lower, adaptive_alpha_upper);
    firefly_lower = sortSpec[16];
    firefly_upper = sortSpec[57];
    adaptive_alpha_lower = sortSpec[1];
    adaptive_alpha_upper = sortSpec[60];
    img_quartiles2[GroupID.xy] = float4(firefly_lower, firefly_upper, adaptive_alpha_lower, adaptive_alpha_upper);
  }
}
